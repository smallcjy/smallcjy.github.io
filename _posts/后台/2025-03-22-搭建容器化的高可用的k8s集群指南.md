---
title: 搭建基于contained的高可用的k8s集群指南
date: 2025-03-22 16:30:31
tags: [后台]
---

# 搭建集群概况
2台2g2g、1台2g4G云服务器实例

集群内服务器角色如下：

| 节点 | 角色 | IP |
| --- | --- | --- |
| k8s-m-1 | master | 192.168.1.1 |
| k8s-m-2 | worker | 192.168.1.2 |
| k8s-m-3 | worker | 192.168.1.3 |
| null | master-vip | 192.168.1.100 |

# 前期准备工作
## 操作系统初始化
- 设置唯一的hostname，uuid，mac
- 保证集群内各节点网络互通，处于同一内网
- 禁用swap
- 使用ntp工具完成时间校对
- 禁用SELinux `sudo setenforce 0 && sudo sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config`
- 设置内核参数

```
cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system
```

## 为master节点安装keepalived和Nginx

`apt install keepalived nginx`

### 配置keepalived.conf

```
global_defs {
    router_id LVS_DEVEL
}
vrrp_script check_apiserver {
  script "/etc/keepalived/check_apiserver.sh"
  interval 3
  weight -2
  fall 10
  rise 2
}

vrrp_instance VI_1 {
    state MASTER
    interface eth0
    virtual_router_id 51
    priority 100
    authentication {
        auth_type PASS
        auth_pass 1111
    }
    virtual_ipaddress {
        192.168.1.100         # 虚拟IP / VIP
    }
    track_script {
        check_apiserver
    }
}
```

其中的check_apiserver.sh的配置如下

```
#!/bin/sh

errorExit() {
    echo "*** $*" 1>&2
    exit 1
}

APISERVER_DEST_PORT=8443
APISERVER_VIP=192.168.0.100

curl --silent --max-time 2 --insecure https://localhost:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit "Error GET https://localhost:${APISERVER_DEST_PORT}/"
if ip addr | grep -q ${APISERVER_VIP}; then
    curl --silent --max-time 2 --insecure https://${APISERVER_VIP}:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit "Error GET https://${APISERVER_VIP}:${APISERVER_DEST_PORT}/"
fi
```

### 配置nginx.conf
```
stream {
    upstream k8s_masters {
        server 192.168.0.1:6443;  # Master 1 的 IP 和 kube-apiserver 端口
    }

    server {
        listen 6443;               # 监听 6443 端口（kube-apiserver 默认端口）
        proxy_pass k8s_masters;     # 将流量转发到 upstream
        proxy_timeout 60s;
        proxy_connect_timeout 60s;
    }
}
```

# 为所有节点安装containerd容器运行环境
## 为所有节点安装containerd容器运行环境（详细指南
### 1. 安装依赖包

```bash
sudo apt-get update
sudo apt-get install -y ca-certificates curl gnupg lsb-release
```

### 2. 添加Docker官方GPG密钥

```bash
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
```

### 3. 添加Docker APT仓库

```bash
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
```

### 4. 安装containerd

```bash
sudo apt-get update
sudo apt-get install -y containerd.io
```

### 5. 配置containerd

创建默认配置文件目录：

```bash
sudo mkdir -p /etc/containerd
```

生成默认配置并保存：

```bash
sudo containerd config default | sudo tee /etc/containerd/config.toml
```

### 6. 修改containerd配置以使用systemd cgroup驱动

编辑配置文件：

```bash
sudo vi /etc/containerd/config.toml
```

找到`[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]`部分，确保`SystemdCgroup`设置为`true`：

```toml
[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
  SystemdCgroup = true
```

### 7. 配置crictl工具（可选但推荐）

```bash
cat <<EOF | sudo tee /etc/crictl.yaml
runtime-endpoint: unix:///run/containerd/containerd.sock
image-endpoint: unix:///run/containerd/containerd.sock
timeout: 10
debug: false
EOF
```

### 8. 配置containerd镜像加速（国内环境建议）

编辑配置文件，找到`[plugins."io.containerd.grpc.v1.cri".registry.mirrors]`部分：

```bash
sudo vi /etc/containerd/config.toml
```

添加镜像加速配置：

```toml
[plugins."io.containerd.grpc.v1.cri".registry.mirrors]
  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
    endpoint = ["https://registry-1.docker.io"]
  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."k8s.gcr.io"]
    endpoint = ["https://registry.k8s.io"]
  # 国内环境可添加以下加速器
  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
    endpoint = ["https://hub-mirror.c.163.com", "https://mirror.baidubce.com"]
```

### 9. 重启containerd并设置开机自启

```bash
sudo systemctl restart containerd
sudo systemctl enable containerd
```

### 10. 验证containerd安装

```bash
sudo systemctl status containerd
```

确保输出显示containerd正在运行（`active (running)`）。

现在，你的所有集群节点都已经安装并配置好了containerd作为容器运行时环境，可以继续进行Kubernetes的安装与配置。

### 为所有节点安装K8s命令行工具

[安装教程](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/)

### 特殊设置
#### 设置Cgroup drivers为systemd
#### 提前下载阿里云镜像
#### 设置SandBox Pause镜像

## 模板化初始化实例
1. 用CentOS7 Minimal官方镜像创建一台虚拟机
2. 网卡的IPv4和DNS使用静态配置
3. yum update -y && yum groupinstall 'Development Tools' -y , 再按照习惯安装其他工具
4. 进行操作系统基础设置: 设置允许免密码登录的rsa公钥, 做好sudoer设置, 禁用firewalld.service, 检查ntpd等等…
5. 安装containerd, K8s命令行工具, 并做好必要的特殊设置
6. 把当前的虚拟机实例设置成模板, 后面k8s集群所有新增的节点都用这个模板进行克隆
克隆出6个节点, 完成必要的调整
7. 为master节点安装keepalived和haproxy, 并妥善配置
8. 为方案中预设的master vip 192.168.0.220设置一个域名: dev.i.k8s.rondochen.com. (因为解析出来只是个内网ip, 就算是一个公网域名, 没有什么安全隐患)
9. 为集群内所有即将要安装k8s的节点做一个快照, 出问题之后方便回滚
10. 上面的步骤都做完之后, 我们即将可以进行集群的初始化了

# K8S集群安装
## 集群初始化
所有节点启动containderd.service
```
systemctl start containerd.service;
systemctl enable containerd.service;
```

k8s-m-1主节点kubeadm初始化
```
kubeadm init \
	--pod-network-cidr=172.16.0.0/16 \
	--service-cidr=10.1.0.0/16 \
	--image-repository registry.aliyuncs.com/google_containers \
	--apiserver-advertise-address 192.168.0.1 \
	--control-plane-endpoint dev.i.k8s.rondochen.com:8443 \
	--upload-certs
```

## 安装calico网络插件
[安装教程](https://projectcalico.docs.tigera.io/getting-started/kubernetes/quickstart)

## 加入更多的节点
在master节点上获取token，然后其他节点调用join命令直接加入即可。

# 接入图形化K8s管理工具

# NCXMALL网站及后台接入集群
## 代接入的所有服务镜像化
自己去编写dockerfile镜像化
## mall网站
mall web是个前端页面，在生产环境下，我们倾向于把项目托管到对象存储中COS，并且在npm run build的时候把页面的资源文件连接也指向对象的存储的地址

