<!-- build time:Thu Nov 28 2024 13:26:56 GMT+0800 (中国标准时间) --><!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#FFF"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png"><link rel="icon" type="image/ico" sizes="32x32" href="/images/favicon.ico"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="alternate" type="application/rss+xml" title="Blog" href="http://smallcjy.github.io/rss.xml"><link rel="alternate" type="application/atom+xml" title="Blog" href="http://smallcjy.github.io/atom.xml"><link rel="alternate" type="application/json" title="Blog" href="http://smallcjy.github.io/feed.json"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Mulish:300,300italic,400,400italic,700,700italic%7CFredericka%20the%20Great:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20JP:300,300italic,400,400italic,700,700italic%7CNoto%20Serif%20SC:300,300italic,400,400italic,700,700italic%7CInconsolata:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/css/app.css?v=0.2.5"><meta name="keywords" content="机器学习"><link rel="canonical" href="http://smallcjy.github.io/2024/09/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><title>机器学习 | SmallC = Blog</title><meta name="generator" content="Hexo 7.2.0"></head><body itemscope itemtype="http://schema.org/WebPage"><div id="loading"><div class="cat"><div class="body"></div><div class="head"><div class="face"></div></div><div class="foot"><div class="tummy-end"></div><div class="bottom"></div><div class="legs left"></div><div class="legs right"></div></div><div class="paw"><div class="hands left"></div><div class="hands right"></div></div></div></div><div id="container"><header id="header" itemscope itemtype="http://schema.org/WPHeader"><div class="inner"><div id="brand"><div class="pjax"><h1 itemprop="name headline">机器学习</h1><div class="meta"><span class="item" title="Created: 2024-09-04 20:51:07"><span class="icon"><i class="ic i-calendar"></i> </span><span class="text">Posted on</span> <time itemprop="dateCreated datePublished" datetime="2024-09-04T20:51:07+08:00">2024-09-04</time></span></div></div></div><nav id="nav"><div class="inner"><div class="toggle"><div class="lines" aria-label="Toggle navigation bar"><span class="line"></span> <span class="line"></span> <span class="line"></span></div></div><ul class="menu"><li class="item title"><a href="/" rel="start">SmallC</a></li></ul><ul class="right"><li class="item theme"><i class="ic i-sun"></i></li><li class="item search"><i class="ic i-search"></i></li></ul></div></nav></div><div id="imgs" class="pjax"><ul><li class="item" data-background-image="https://s21.ax1x.com/2024/07/18/pkoWXdO.jpg"></li><li class="item" data-background-image="https://s21.ax1x.com/2024/07/18/pkoWTzR.jpg"></li><li class="item" data-background-image="https://s21.ax1x.com/2024/07/18/pkoWqL6.png"></li><li class="item" data-background-image="https://s21.ax1x.com/2024/07/18/pkoWjoD.png"></li><li class="item" data-background-image="https://s21.ax1x.com/2024/07/18/pkoWxFe.png"></li><li class="item" data-background-image="https://s21.ax1x.com/2024/07/18/pkoWHQ1.png"></li></ul></div></header><div id="waves"><svg class="waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z"/></defs><g class="parallax"><use xlink:href="#gentle-wave" x="48" y="0"/><use xlink:href="#gentle-wave" x="48" y="3"/><use xlink:href="#gentle-wave" x="48" y="5"/><use xlink:href="#gentle-wave" x="48" y="7"/></g></svg></div><main><div class="inner"><div id="main" class="pjax"><div class="article wrap"><div class="breadcrumb" itemscope itemtype="https://schema.org/BreadcrumbList"><i class="ic i-home"></i> <span><a href="/">Home</a></span></div><article itemscope itemtype="http://schema.org/Article" class="post block" lang="en"><link itemprop="mainEntityOfPage" href="http://smallcjy.github.io/2024/09/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.jpg"><meta itemprop="name" content="Cai Junyuan"><meta itemprop="description" content=", "></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="Blog"></span><div class="body md" itemprop="articleBody"><h1 id="machine-learning"><a class="anchor" href="#machine-learning">#</a> Machine Learning</h1><p>基本要素：</p><ul><li>Data：输入，不同的应用有不同的数据</li><li>Model：类似于函数</li><li>Loss Function：抽象的本质的模型</li></ul><p>约等于 Looking for a Function</p><p>steps：</p><ol><li>定义函数集</li><li>给函数评分（Goodness）</li><li>选择最好的函数</li></ol><ul><li>对于预测 y 有一个 function：y=f（x） --》 数据</li><li>Ld (W) = l (yi', yi) 的全数据之和 --》 模型</li></ul><h2 id="data数据集"><a class="anchor" href="#data数据集">#</a> Data 数据集</h2><p><code>D = &#123;(xi, yi)&#125;n i=1</code></p><p>x 是输入，使用列向量表示；</p><p>y 是输出；</p><p>n 是数据的个数</p><h2 id="损失函数loss-function"><a class="anchor" href="#损失函数loss-function">#</a> 损失函数 Loss function</h2><p>对值的预测：</p><p><img data-src="image.png" alt="predict y"><br>损失函数定义：数据集中的所有数据值的预测值和真实值之间的差值的求和</p><p><img data-src="image-1.png" alt="LossFunction"></p><h2 id="supervised-learning-监督学习"><a class="anchor" href="#supervised-learning-监督学习">#</a> Supervised Learning 监督学习</h2><p>使用 labeled training data 来训练模型，模型的目标是预测新的数据的输出值。监督学习的特征是训练数据是带有标签的数据。</p><h2 id="unsupervised-learning-无监督学习"><a class="anchor" href="#unsupervised-learning-无监督学习">#</a> Unsupervised Learning 无监督学习</h2><p>Learning the inner connection between different training samples , but without specific labels/</p><ul><li>模型通过比较得到数据集隐藏的结构</li><li>K-means clustering</li></ul><h2 id="reinforcement-learning-强化学习"><a class="anchor" href="#reinforcement-learning-强化学习">#</a> Reinforcement Learning 强化学习</h2><p>强化学习包括下面几个部分：</p><ul><li>Agent：智能体，决策者</li><li>Environment：环境，智能体所处的环境</li><li>Action：智能体在环境中的行为</li><li>Reward：智能体在环境中的行为的反馈</li><li>State：智能体在环境中的状态</li></ul><h1 id="1-linear-regression-and-gradient-descent线性回归与梯度下降"><a class="anchor" href="#1-linear-regression-and-gradient-descent线性回归与梯度下降">#</a> 1 Linear Regression and Gradient Descent 线性回归与梯度下降</h1><p>Challenges in Learning: Uncertainty and Unlinear<br>线性回归是一种线性模型，通过线性回归模型可以预测连续值的输出。线性回归模型的目标是找到一条直线，使得数据集中的所有数据点到直线的距离之和最小。线性回归模型的损失函数是均方误差，均方误差是预测值和真实值之间的差值的平方的求和。</p><p>两种损失函数：<br>absolute loss function：L(y', y) = |y' - y|<br>squared loss function：L(y', y) = (y' - y)^2 / 2<br>Total loss function：</p><p><img data-src="image-2.png" alt="total loss function"></p><h2 id="regression"><a class="anchor" href="#regression">#</a> Regression</h2><p>Find the best f by solvig the following optimization problem:</p><p><img data-src="image-3.png" alt="最小优化问题"><br>即找到一个函数 f 使得 total loss function 最小</p><p>这个 f 如何定义呢？</p><p><img data-src="image-4.png" alt="model function"><br>找到合适的 W 向量和 b 值使得 total loss function 最小<br>根据这个 f 可以将 total loss function 转化为：</p><p><img data-src="image-5.png" alt="new total loss function"></p><p><img data-src="C:%5CUsers%5C26280%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5Cimage-20241127201729420.png" alt="image-20241127201729420"></p><p>那么如何找到这个 W 和 b 呢？</p><h2 id="closed-form-solution"><a class="anchor" href="#closed-form-solution">#</a> Closed-form Solution</h2><p>闭式解也称为解析解</p><p>solution:</p><p><img data-src="image-6.png" alt="closed-form"></p><h2 id="regularized-least-squares-regression"><a class="anchor" href="#regularized-least-squares-regression">#</a> Regularized Least Squares Regression</h2><p><img data-src="image-7.png" alt="RLS"><br>𝜆 is called trade-off parameter or regularization parameter</p><p>得到的 solution：<br><img data-src="image-8.png" alt="RLS solution"></p><h2 id="gradient-descent-随机梯度下降"><a class="anchor" href="#gradient-descent-随机梯度下降">#</a> Gradient Descent 随机梯度下降</h2><p>原理：通过不断在损失函数递减的方向上更新参数来降低误差，由于每次更新 w 参数需要在全部数据集上更新计算，性能低。</p><p><img data-src="image-9.png" alt="algorithm"></p><p>由此出现随机梯度算法：</p><h3 id="随机梯度算法"><a class="anchor" href="#随机梯度算法">#</a> 随机梯度算法</h3><p>每次更新只选取一小部分数据集来更新 w</p><p><img data-src="./image-20241127201959360.png" alt="image-20241127201959360"></p><p>n 表示学习率</p><h1 id="2-linear-classifcation-and-stochastic-gradient-descent-线性分类-支持向量机-随机梯度算法"><a class="anchor" href="#2-linear-classifcation-and-stochastic-gradient-descent-线性分类-支持向量机-随机梯度算法">#</a> 2 Linear Classifcation and Stochastic Gradient Descent 线性分类、支持向量机、随机梯度算法</h1><h2 id="binary-classification-二元分类"><a class="anchor" href="#binary-classification-二元分类">#</a> Binary Classification 二元分类</h2><p>（二元分类）是机器学习中的一种基本任务。它指的是将数据分为两个类别之一，例如 “是 / 否”、“成功 / 失败” 或 “正类 / 负类”。</p><h2 id="support-vector-machine-支持向量机"><a class="anchor" href="#support-vector-machine-支持向量机">#</a> support vector machine 支持向量机</h2><h4 id="超平面和间隔"><a class="anchor" href="#超平面和间隔">#</a> 超平面和间隔</h4><p>一个超平面由法向量 W 和截距 b 决定，其方程为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mi>T</mi></msup><mi>W</mi><mo>+</mo><mi>b</mi><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">X^TW + b = 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.924661em;vertical-align:-.08333em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.07847em">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8413309999999999em"><span style="top:-3.063em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:.13889em">W</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal">b</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:.64444em;vertical-align:0"></span><span class="mord">0</span></span></span></span>。</p><p><strong>什么是 good decision boundary？</strong><br>一个好的决策边界是在分类问题中能够有效区分不同类别的数据点，并对未见数据具有良好泛化能力的边界。</p><p>Maximum margin solution（最大化间隔解）是一种在分类任务中追求分类器稳健性的策略，尤其常见于支持向量机（SVM）中。选择两个平行的超平面，使其距离间隔经可能大。</p><p>二元分类的目标函数 f，选择两条平行的线，使得两条线之间的距离最大，这两条线就是支持向量机的决策边界。同时这两条平行线仍然需要满足能够区分数据集中的正类和负类数据点。这两条线的距离就称为 margin（间隔）。</p><p><img data-src="image.png" alt="max-margin"></p><p><strong>SVM-sketch Derivation</strong></p><p>在支持向量机中，分类器的目标是找到一个决策边界（超平面 y = wx + b）, 支持向量是离超平面最近的点，这些点决定了分类边界的位置。</p><p><img data-src="image-1.png" alt="normalization"></p><p>已知归一化后的正负类支持向量平面，求解两平面之间的距离，即 margin。</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mrow><mi>m</mi><mi>a</mi><mi>r</mi><mi>g</mi><mi>i</mi><mi>n</mi></mrow></msub><mo>=</mo><mn>2</mn><mi mathvariant="normal">/</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi><mi>w</mi><mi mathvariant="normal">∣</mi><mi mathvariant="normal">∣</mi></mrow><annotation encoding="application/x-tex">d_{margin} = 2 / ||w||</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.980548em;vertical-align:-.286108em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.311664em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:.02778em">r</span><span class="mord mathnormal mtight" style="margin-right:.03588em">g</span><span class="mord mathnormal mtight">i</span><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.286108em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">2</span><span class="mord">/</span><span class="mord">∣</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="mord">∣</span><span class="mord">∣</span></span></span></span></p><p><strong>SVM 是对 margin 的最大化</strong><br><img data-src="./image-20241127203625773.png" alt="image-20241127203625773"></p><p><strong>Relaxed Formulation 松弛形式</strong><br>Relaxed Formulation（松弛形式）通常指在支持向量机（SVM）或其他优化问题中，为了处理数据非线性可分的情况，对原始的硬性约束（Hard Margin Constraints）进行放松，从而提出的改进版本。</p><p>在 SVM 中，这种松弛形式被称为 ** 软 ** 支持向量机（Soft Margin SVM）。</p><p>引入松弛模型后，目标函数变为：</p><p><img data-src="./image-20241127203943589.png" alt="image-20241127203943589"></p><p>C&gt;0 被称为惩罚参数，C 越小时对误分类参数惩罚越小，反之越大。</p><p>引入松弛变量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ξ</mi><mi>i</mi></mrow><annotation encoding="application/x-tex">ξi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.8888799999999999em;vertical-align:-.19444em"></span><span class="mord mathnormal" style="margin-right:.04601em">ξ</span><span class="mord mathnormal">i</span></span></span></span> &gt;=0，则目标函数又可变为：</p><p><img data-src="./image-20241127204212970.png" alt="image-20241127204212970"></p><h2 id="gradient-descent-梯度下降"><a class="anchor" href="#gradient-descent-梯度下降">#</a> Gradient Descent 梯度下降</h2><p>梯度下降是一种优化算法，用于寻找函数的最小值。在机器学习中，梯度下降被广泛应用于训练模型，例如线性回归、逻辑回归和神经网络。</p><p><img data-src="image-6.png" alt="gradient descent update"></p><p><strong>Gradient Descent Algorithm</strong><br><img data-src="image-7.png" alt="gradient descent algorithm1"></p><p><img data-src="image-8.png" alt="gradient descent algorithm2"></p><p><img data-src="image-9.png" alt="subgradient"></p><h2 id="stochastic-gradient-descent-随机梯度下降"><a class="anchor" href="#stochastic-gradient-descent-随机梯度下降">#</a> stochastic gradient descent 随机梯度下降</h2><p>随机梯度下降是梯度下降的一种变体。与梯度下降不同，随机梯度下降在每次迭代中只使用一个样本来计算梯度。这使得随机梯度下降在处理大规模数据集时更加高效。</p><p><img data-src="image-10.png" alt="stochastic"></p><p><strong>Minibatch Stochastic Gradient Descent</strong></p><p>Mini-Batch Stochastic Gradient Descent (Mini-Batch SGD) 是 随机梯度下降（SGD） 的一种改进版本，它结合了 批量梯度下降（Batch Gradient Descent） 和 随机梯度下降（SGD） 的优点。与传统的 SGD 每次仅使用一个样本来更新模型参数不同，Mini-Batch SGD 使用小批量数据（mini-batch）来进行更新。</p><p><img data-src="image-11.png" alt="mini-batch"></p><h1 id="3-logistic-regression-and-ensemble-methods-逻辑回归与集成学习算法"><a class="anchor" href="#3-logistic-regression-and-ensemble-methods-逻辑回归与集成学习算法">#</a> 3 Logistic Regression and Ensemble Methods 逻辑回归与集成学习算法</h1><p>在学习逻辑回归之前，先来了解机器学习的原则目标： Prediction 预测与 Making Decisions 决策</p><p><img data-src="image-12.png" alt="pipeline of data-driven methods"></p><h2 id="logistic-regression-逻辑回归"><a class="anchor" href="#logistic-regression-逻辑回归">#</a> Logistic Regression 逻辑回归</h2><p>逻辑回归（Logistic Regression） 是一种常见的分类算法，尽管它的名字中包含 “回归”，但它实际上用于分类问题，尤其是二分类问题。它基于统计学和概率论的原理，通过将输入特征映射到一个概率值来预测类别。</p><p><strong>逻辑回归（Logistic Regression）</strong> 是一种常见的分类算法，尽管它的名字中包含 “回归”，但它实际上用于<strong>分类问题</strong>，尤其是<strong>二分类问题</strong>。它基于统计学和概率论的原理，通过将输入特征映射到一个概率值来预测类别。以下是对逻辑回归的详细讲解：</p><hr><h3 id="1-逻辑回归的基本原理"><a class="anchor" href="#1-逻辑回归的基本原理">#</a> 1. <strong>逻辑回归的基本原理</strong></h3><p>逻辑回归试图找到一个最佳的决策边界，用于将输入数据分为两个类别（例如，正类和负类）。它的目标是计算给定输入特征 x 时，属于某一类别（通常是类别 1）的概率。所以逻辑回归算法的输出值永远在 0 到 1 之间。</p><h4 id="假设"><a class="anchor" href="#假设">#</a> 假设</h4><ul><li>假设我们有一个输入向量 x = [x_1, x_2, ..., x_n]，这是我们的特征向量。</li><li>y 是目标变量，通常为二元标签，表示类别：(y in {0, 1} )。</li></ul><h4 id="逻辑回归模型"><a class="anchor" href="#逻辑回归模型">#</a> 逻辑回归模型</h4><p>逻辑回归的模型基于线性回归模型的输出，但通过一个 **sigmoid 函数（logistic 函数）** 进行转换，以确保输出值在 0 和 1 之间，可以解释为概率。</p><p>模型的形式为：</p><p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mo stretchy="false">[</mo><mi>p</mi><mo stretchy="false">(</mo><mi>y</mi><mo>=</mo><mn>1</mn><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mi>T</mi></msup><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo><mo stretchy="false">]</mo></mrow><annotation encoding="application/x-tex">[ p(y = 1 | x) = sigma(w^T x + b) ]</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">[</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:.03588em">y</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.1413309999999999em;vertical-align:-.25em"></span><span class="mord mathnormal">s</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:.03588em">g</span><span class="mord mathnormal">m</span><span class="mord mathnormal">a</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:.8913309999999999em"><span style="top:-3.113em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord mathnormal">b</span><span class="mclose">)</span><span class="mclose">]</span></span></span></span></span></p><p>其中：<br><img data-src="image-13.png" alt="wenben"></p><h4 id="目标"><a class="anchor" href="#目标">#</a> 目标</h4><ul><li>通过训练学习参数 w 和 b，使得模型能够尽可能准确地预测样本属于类别 1 的概率。</li></ul><hr><h3 id="2-损失函数与优化"><a class="anchor" href="#2-损失函数与优化">#</a> 2. <strong>损失函数与优化</strong></h3><p>逻辑回归通过最小化<strong>对数似然函数（log-likelihood）<strong>来估计参数 (w) 和 ( b )。对于二分类问题，损失函数通常采用</strong>交叉熵损失（cross-entropy loss）</strong>，其公式为：</p><p><img data-src="image-16.png" alt="gongshi"></p><p>损失函数的目标是最小化交叉熵损失，意味着我们尽可能将预测概率 p 和真实标签 y 对应的概率接近。</p><h4 id="优化方法"><a class="anchor" href="#优化方法">#</a> 优化方法</h4><ul><li><strong>梯度下降</strong>：通常使用梯度下降算法来优化参数 (w) 和 ( b )。</li><li><strong>梯度计算</strong>：<br>[<br>\frac<ruby>\partial J(w, b)}{\partial w} = \frac{1}{m} \sum_{i=1}<rp>(</rp><rt>{m</rt><rp>)</rp></ruby>(p^<ruby>(i)} - y<rp>(</rp><rt>{(i)</rt><rp>)</rp></ruby>) x^<ruby>(i)} \] \[ \frac{\partial J(w, b)}{\partial b} = \frac{1}{m} \sum_{i=1}<rp>(</rp><rt>{m</rt><rp>)</rp></ruby>(p^<ruby>(i)} - y<rp>(</rp><rt>{(i)</rt><rp>)</rp></ruby>)<br>]<br>然后根据这些梯度更新参数 (w) 和 ( b )。</li></ul><hr><h3 id="3-模型评估与决策边界"><a class="anchor" href="#3-模型评估与决策边界">#</a> 3. <strong>模型评估与决策边界</strong></h3><p>逻辑回归输出的是类别 1 的概率，通常情况下，我们根据某个阈值（例如 0.5）将其转换为具体的类别预测：</p><ul><li>如果 (p (y = 1 | x) &gt; 0.5 )，则预测为类别 1。</li><li>如果 (p (y = 1 | x) &lt; 0.5 )，则预测为类别 0。</li></ul><h4 id="决策边界"><a class="anchor" href="#决策边界">#</a> 决策边界</h4><p>逻辑回归的决策边界是一个超平面（线性边界），可以通过求解：<br>[<br>w^T x + b = 0<br>]<br>来得到。这个边界将数据空间划分为两个区域，预测值大于 0.5 的区域为类别 1，反之为类别 0。</p><hr><h3 id="4-正则化"><a class="anchor" href="#4-正则化">#</a> 4. <strong>正则化</strong></h3><p>为了防止过拟合，逻辑回归通常采用<strong>正则化</strong>方法，特别是 L2 正则化（也叫 Ridge 正则化）。通过在损失函数中加入正则化项，可以限制模型的复杂度，避免过度拟合训练数据。</p><p>带正则化的损失函数为：<br><img data-src="image-17.png" alt="regularization"></p><p>Logistic Loss:<br><img data-src="image-18.png" alt="logistic loss"></p><p>正则化的目标是平衡模型的复杂度和训练数据的拟合程度，防止过拟合。</p><h3 id="总结"><a class="anchor" href="#总结">#</a> 总结</h3><p>逻辑回归是一种简单、有效的分类方法，广泛应用于二分类和多分类问题。通过最大化对数似然函数，它能够估计出给定特征时属于某一类别的概率。理解其背后的原理和优化方法有助于在实际应用中有效地使用该模型，同时结合正则化可以避免过拟合，提高泛化能力。</p><p>如果你有其他问题或需要更具体的代码示例，可以进一步提问！</p><h2 id="多类别分类问题的定义"><a class="anchor" href="#多类别分类问题的定义">#</a> 多类别分类问题的定义</h2><ul><li><p>one vs rest method</p></li><li><p>one vs one method</p></li><li><p>DDAG 决策有向无环图<br>根二叉有向无环图（Rooted Binary Directed Acyclic Graph，简称 Rooted Binary DAG）是一种特殊类型的有向无环图（DAG），具有以下特点：</p></li></ul><p>根节点（Root Node）：图中有一个唯一的根节点，所有其他节点都可以通过有向路径从根节点到达。</p><p>二叉性质（Binary Property）：每个节点最多有两个子节点。</p><p>有向无环（Directed Acyclic）：图中的边是有方向的，并且不存在从某个节点出发经过若干条边又回到该节点的路径，即图中没有环。</p><p>这种结构常用于表示决策树、二叉堆等数据结构。</p><p>预测过程：<br><img data-src="image-19.png" alt="ddag"></p><ul><li>K-means algorithm K 均值算法<br>K 均值算法（K-means algorithm）是一种常用的聚类算法，用于将数据集分成 K 个簇。其处理过程如下：</li></ul><p>初始化：随机选择 K 个初始质心（Centroids）。</p><p>分配簇：对于数据集中的每个数据点，计算其到每个质心的距离。<br>将每个数据点分配到距离其最近的质心所对应的簇。</p><p>更新质心：计算每个簇中所有数据点的平均值，并将该平均值作为新的质心。</p><p>重复：重复步骤 2 和步骤 3，直到质心不再发生变化或达到预设的迭代次数。</p><p>结果：最终得到 K 个簇，每个簇由距离其质心最近的数据点组成。</p><h1 id="4-overfitting-underfitting-regularization-and-cross-validation-过拟合-欠拟合-正则化和交叉验证"><a class="anchor" href="#4-overfitting-underfitting-regularization-and-cross-validation-过拟合-欠拟合-正则化和交叉验证">#</a> 4 Overfitting, Underfitting, Regularization and Cross-Validation 过拟合、欠拟合、正则化和交叉验证</h1><p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream: 机器学习 / 机器学习.md<br>先回顾一下机器学习模型训练的过程:<br><img data-src="image-20.png" alt="model train"><br>判断 model 的质量需要使用 loss function，loss function 的值越小，model 的质量越好。</p><p>loss function 有三种类型：</p><ul><li>Hinge Loss:<br><img data-src="image-21.png" alt="hinge loss"></li><li>Logistic Loss:<br><img data-src="image-22.png" alt="logistic loss"></li><li>soft-max loss:<br><img data-src="image-23.png" alt="soft-max"></li></ul><p>知道这几种 loss 函数的定义，那我们又该如何使用这些 loss 函数来判断 model 的质量呢？我们可以使用中学时期学到的最小方差损失判定，但是很多时候模型并不是线性的，所以我们需要使用交叉验证的方法来判断模型的质量。</p><p>首先介绍两种集： 训练集和测试集</p><ul><li>训练集：用于拟合模型的数据（70%）</li><li>测试集：用于评估模型的泛化能力（30%）</li></ul><p>在训练集中又被分隔出一块：交叉验证集。为什么需要交叉验证集 validation set 呢</p><ul><li>Business Reason：<ul><li>Need to choose the beat model</li><li>measure accuracy（准确性）/power of the selected model</li><li>better to measure ROI of the modeling project</li></ul></li><li>Statistical Reason：<ul><li>model building techniques are inherently designed to minimize loss or bias</li><li>to an extent, a model will always fit &quot;noise&quot; as well as signal</li><li>if you just fit a bunch of models on a givne dataset and choose the &quot;best&quot; one , it will likely be overly optimistic</li></ul></li></ul><h3 id="过拟合与欠拟合"><a class="anchor" href="#过拟合与欠拟合">#</a> 过拟合与欠拟合</h3><p>UnderFitting：模型无法捕捉到数据的底层趋势</p><p>OverFitting：模型完美拟合数据，但是也把噪音纳入进来，导致表现形式很繁琐</p><h3 id="偏差-方差权衡-bias-variance-tradeoff"><a class="anchor" href="#偏差-方差权衡-bias-variance-tradeoff">#</a> 偏差 - 方差权衡 Bias-Variance Tradeoff</h3><p>对于复杂的模型，可以忽略这个模型再未来数据的准确性。<br><img data-src="image-24.png" alt="complex model"></p><h3 id="交叉验证"><a class="anchor" href="#交叉验证">#</a> 交叉验证</h3><p>使用 validation set 以调优高维参数，然后仅使用 test set 来评估模型的泛化能力。</p><p>如果我们想减少数据的可变性：</p><ul><li>使用不同分区进行多轮交叉验证</li><li>对所有轮次的结果进行平均</li></ul><h1 id="5-非线性机器学习与集成方法"><a class="anchor" href="#5-非线性机器学习与集成方法">#</a> 5 非线性机器学习与集成方法</h1><h2 id="决策树"><a class="anchor" href="#决策树">#</a> 决策树</h2><p>定义：树的分支代表一个可能的决策、结果或者反应，现实表现为父节点的属性值。中间节点表示某种属性，叶子节点表示最终结果。<br>那么如何划分决策才是最优的呢？有个评判标准：对于一个 value，可以得到全为正的实例，而其余的 value，可以得到全为负的实例。如果划分后的每一片叶子的区分度都足够大，说明划分方式好。Entropy 熵就是用于表明区分度的。</p><p>必考：如何求解数据集合 D 的熵 呢？<br>口诀：负数、比例 p、Alog2A</p><p>假设叶子节点内部有 9 个正数节点和 5 个负数节点，可以根据口诀退出熵计算公式：</p><p><code>entropy(P) = - p+log2p+ - p-log2p-</code></p><p>当然叶子节点内部也不一定只有正负两个类别，我们假设有 c 个类别，那么熵的计算公式为：</p><p><code>entropy(D) = - sum(pclog2pc)</code></p><p>下面再来介绍 Gain，Gain 用于表示对于某属性 A，其分类可以获得的熵信息增益<br><code>Gain(D, A) = Entropy(D) - sum(* Entropy(Dv))</code></p><ul><li>v 属于属性 A 可以取到的所有可能值的集合</li><li>Dv 是 D 的子集，表示 A 的值为 v</li></ul><h3 id="示范把类似打网球的表格转化为决策树"><a class="anchor" href="#示范把类似打网球的表格转化为决策树">#</a> 示范：把类似打网球的表格转化为决策树</h3><p><img data-src="image-25.png" alt="decision tree"><br>思路：</p><ol><li>四颗树木（可能的决策树）套公式计算信息密度，发现 Outlook 的 Gain 值最大</li><li>Overcast 的中间节点没必要拆分（因为叶子节点的值全为正的），只需要拆分 sunny 和 rain 对应</li><li>对于 sunny 节点，我们可以使用 humidity、wind、temp、继续拆分。</li><li>对于 Rain 节点，同样使用 humidity、wind、temp、继续拆分、wind 拆分效果最好</li></ol><ul><li>低偏差：模型在训练集拟合得好</li><li>高方差：模型更有可能做出错误的决策</li></ul><h1 id="5-multiclass-classification-and-cross-entropy-loss-多类分类和交叉熵损失函数"><a class="anchor" href="#5-multiclass-classification-and-cross-entropy-loss-多类分类和交叉熵损失函数">#</a> 5 Multiclass Classification and Cross-entropy Loss 多类分类和交叉熵损失函数</h1><h1 id="6-neural-networks-and-deep-learning-神经网络与深度学习"><a class="anchor" href="#6-neural-networks-and-deep-learning-神经网络与深度学习">#</a> 6 Neural Networks and Deep Learning 神经网络与深度学习</h1><h1 id="7-rnn-transformer-bert"><a class="anchor" href="#7-rnn-transformer-bert">#</a> 7 RNN、Transformer、BERT</h1><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><blockquote><p>Stashed changes: 机器学习.md</p></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote></blockquote><h2 id="集成学习"><a class="anchor" href="#集成学习">#</a> 集成学习</h2><p>将一系列基础模型合并到一起，从而产生一个更好的预测模型。主要方法：Bagging、Boosting</p><h3 id="bagging"><a class="anchor" href="#bagging">#</a> Bagging</h3><ul><li>通过 bootstrap sampling（重采样）得到 T 个数据集</li><li>训练 T 个 base learners 通过子数据集</li></ul><p>有两种得到结果的方法：</p><ul><li>Majority Voting：对于分类问题，选择 T 个模型中出现最多次数的结果</li><li>Average：对于回归问题，选择 T 个模型的平均值</li></ul><p>随机森林是 Bagging 的一种实现，它是一种集成学习方法，通过训练多个决策树来提高预测性能。随机森林的基本原理是通过随机选择特征和样本来构建多个决策树，并将这些决策树的预测结果进行平均或投票来得到最终的预测结果。</p><h2 id="adaboostadaptive-boosting"><a class="anchor" href="#adaboostadaptive-boosting">#</a> AdaBoost（Adaptive Boosting）</h2><p>原理：通过训练一系列弱分类器（weak learners），每个弱分类器都在前一个分类器的错误上进行学习，最终将这些弱分类器组合成一个强分类器（strong learner）。</p><p>进行 Time 次循环的时候，每次选择在当前权重下，错误率最小的弱分类机。</p><p><img data-src="./image-20241128103036889.png" alt="image-20241128103036889"></p><h1 id="6-无监督学习聚类学习"><a class="anchor" href="#6-无监督学习聚类学习">#</a> 6 无监督学习：聚类学习</h1><p>无监督学习的数据集是没有目标值的。</p><h2 id="重点k-means聚类"><a class="anchor" href="#重点k-means聚类">#</a> 重点：K-Means 聚类</h2><ul><li>过程：</li></ul><ol><li>随机选取 K 个点作为初始质心（centroid）</li><li>将每个点分配到最近的质心</li><li>更新质心</li><li>重复 2 和 3 直到质心不再变化或者数据点分配情况基本不变</li></ol><h2 id="层次聚合聚类-hierarchical-agglomerative-clustering"><a class="anchor" href="#层次聚合聚类-hierarchical-agglomerative-clustering">#</a> 层次聚合聚类 Hierarchical Agglomerative Clustering</h2><p>层次聚合聚类（Hierarchical Agglomerative Clustering，HAC）是一种无监督学习算法，用于将数据点分层次地聚类。HAC 的基本思想是通过不断合并最相似的簇来构建一个层次结构，直到所有数据点都被合并到一个簇中。以下是 HAC 的详细过程：</p><ol><li><strong>初始化</strong>：将每个数据点视为一个单独的簇。</li><li><strong>计算距离</strong>：计算所有簇之间的距离。常用的距离度量方法包括欧氏距离、曼哈顿距离等。</li><li><strong>合并簇</strong>：找到距离最近的两个簇，并将它们合并成一个新的簇。</li><li><strong>更新距离矩阵</strong>：更新距离矩阵，重新计算新簇与其他簇之间的距离。常用的更新方法包括最小距离（单链接）、最大距离（全链接）、平均距离和质心距离等。</li><li><strong>重复步骤 3 和 4</strong>：重复合并和更新距离矩阵的过程，直到所有数据点都被合并到一个簇中，或者达到预定的簇数量。</li></ol><p>HAC 的结果通常用树状图（dendrogram）表示，树状图展示了数据点的合并过程和层次结构。通过剪切树状图，可以得到不同数量的簇。</p><p>HAC 的优点是能够发现数据的层次结构，不需要预先指定簇的数量。缺点是计算复杂度较高，适用于小规模数据集。</p><ul><li>与 K-means 算法的比较：<ul><li>不是参数依赖的，而是实例依赖性</li><li>可以生成任意形状的聚类</li><li>生成层级的聚类，聚类不只是 “在平面切分的”</li><li>不需要在聚类之前指定聚类的数量，也不是随机的</li></ul></li></ul><h1 id="7-neural-networks-and-deep-learning-神经网络与深度学习"><a class="anchor" href="#7-neural-networks-and-deep-learning-神经网络与深度学习">#</a> 7 Neural Networks and Deep Learning 神经网络与深度学习</h1><p><img data-src="image-27.png" alt="neural network"><br>使用权重 w 对 x 进行线性变换后带入激活函数得到结果，重复多次的过程是神经网络的基本原理。</p><h2 id="前向传播"><a class="anchor" href="#前向传播">#</a> 前向传播</h2><p><img data-src="image-26.png" alt="Lossfunction set"></p><h2 id="反向传播"><a class="anchor" href="#反向传播">#</a> 反向传播</h2><h2 id="cnn-卷积神经网络"><a class="anchor" href="#cnn-卷积神经网络">#</a> CNN 卷积神经网络</h2><p><em>卷积神经网络</em>（convolutional neural networks，CNN）是机器学习利用自然图像中一些已知结构的创造性方法。卷积神经网络是将<em>空间不变性</em>（spatial invariance）的这一概念系统化，从而基于这个模型使用较少的参数来学习有用的表示。</p><ol><li><em>平移不变性</em>（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为 “平移不变性”。</li><li><em>局部性</em>（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是 “局部性” 原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</li></ol><h3 id="图像卷积操作"><a class="anchor" href="#图像卷积操作">#</a> 图像卷积操作</h3><p>卷积层所表达的运算实际是互相关运算（cross-correlation），而不是卷积运算。输入张量和核张量通过互相关运算产生输出张量。</p><p>已知输入张量为 nhXnw，卷积核为 khXkw，得到输出张量的规格：</p><p><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>n</mi><mi>h</mi></msub><mo>−</mo><msub><mi>k</mi><mi>h</mi></msub><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>∗</mo><mo stretchy="false">(</mo><msub><mi>n</mi><mi>w</mi></msub><mo>−</mo><msub><mi>k</mi><mi>w</mi></msub><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(n_h - k_h + 1)*(n_w-k_w+1)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-.03148em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:0;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">−</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:.84444em;vertical-align:-.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:.03148em">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:.151392em"><span style="top:-2.5500000000000003em;margin-left:-.03148em;margin-right:.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:.02691em">w</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-.25em"></span><span class="mord">1</span><span class="mclose">)</span></span></span></span></p><p>实例：</p><p><img data-src="./image-20241127213926417.png" alt="image-20241127213926417"></p><ul><li>Stride：卷积核移动的步长</li><li>Padding：在输入数据周围填充额外的数据</li></ul><h1 id="8-rnn-transformer-bert"><a class="anchor" href="#8-rnn-transformer-bert">#</a> 8 RNN、Transformer、BERT</h1><h2 id="强化学习"><a class="anchor" href="#强化学习">#</a> 强化学习</h2><p>三个阶段：监督训练、奖励模型训练、强化学习模型训练</p><ul><li><p>监督训练：从数据集中采样问题、根据采样问题，人工给出高质量回答、基于问题和回答，对 gpt 进行监督训练</p></li><li><p>奖励模型训练：从数据集采样问题并输入第一阶段训练的 gpt 模型，根据问题和回答，使用奖励模型给出奖励，基于问题和回答，对 gpt 进行奖励训练</p></li><li><p>强化学习模型训练：从数据集采样问题并输入第一阶段训练的 gpt 模型，根据问题和回答，使用强化学习模型给出奖励，基于问题和回答，对 gpt 进行强化学习训练</p></li></ul><div class="tags"><a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="ic i-tag"></i> 机器学习</a></div></div><footer><div class="meta"><span class="item"><span class="icon"><i class="ic i-calendar-check"></i> </span><span class="text">Edited on</span> <time title="Modified: 2024-11-28 10:51:09" itemprop="dateModified" datetime="2024-11-28T10:51:09+08:00">2024-11-28</time></span></div><div class="reward"><button><i class="ic i-heartbeat"></i> Donate</button><p>Give me a cup of [coffee]~(￣▽￣)~*</p><div id="qr"><div><img data-src="/images/wechatpay.png" alt="Cai Junyuan WeChat Pay"><p>WeChat Pay</p></div><div><img data-src="/images/alipay.png" alt="Cai Junyuan Alipay"><p>Alipay</p></div><div><img data-src="/images/paypal.png" alt="Cai Junyuan PayPal"><p>PayPal</p></div></div></div><div id="copyright"><ul><li class="author"><strong>Post author: </strong>Cai Junyuan <i class="ic i-at"><em>@</em></i>Blog</li><li class="link"><strong>Post link: </strong><a href="http://smallcjy.github.io/2024/09/04/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" title="机器学习">http://smallcjy.github.io/2024/09/04/机器学习/</a></li><li class="license"><strong>Copyright Notice: </strong>All articles in this blog are licensed under <span class="exturl" data-url="aHR0cHM6Ly9jcmVhdGl2ZWNvbW1vbnMub3JnL2xpY2Vuc2VzL2J5LW5jLXNhLzQuMC9kZWVkLnpo"><i class="ic i-creative-commons"><em>(CC)</em></i>BY-NC-SA</span> unless stating additionally.</li></ul></div></footer></article></div><div class="post-nav"><div class="item left"><a href="/2024/08/15/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4%E6%80%BB%E7%BB%93/" itemprop="url" rel="prev" data-background-image="https:&#x2F;&#x2F;s21.ax1x.com&#x2F;2024&#x2F;07&#x2F;18&#x2F;pkoWOeK.png" title="大二学年总结"><span class="type">Previous Post</span> <span class="category"><i class="ic i-flag"></i></span><h3>大二学年总结</h3></a></div><div class="item right"><a href="/2024/09/04/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" itemprop="url" rel="next" data-background-image="https:&#x2F;&#x2F;s21.ax1x.com&#x2F;2024&#x2F;07&#x2F;18&#x2F;pkoWOeK.png" title="数据库系统"><span class="type">Next Post</span> <span class="category"><i class="ic i-flag"></i></span><h3>数据库系统</h3></a></div></div><div class="wrap" id="comments"></div></div><div id="sidebar"><div class="inner"><div class="panels"><div class="inner"><div class="contents panel pjax" data-title="Contents"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#machine-learning"><span class="toc-number">1.</span> <span class="toc-text">Machine Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#data%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">1.1.</span> <span class="toc-text">Data 数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0loss-function"><span class="toc-number">1.2.</span> <span class="toc-text">损失函数 Loss function</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#supervised-learning-%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.3.</span> <span class="toc-text">Supervised Learning 监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#unsupervised-learning-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.4.</span> <span class="toc-text">Unsupervised Learning 无监督学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reinforcement-learning-%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">1.5.</span> <span class="toc-text">Reinforcement Learning 强化学习</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-linear-regression-and-gradient-descent%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.</span> <span class="toc-text">1 Linear Regression and Gradient Descent 线性回归与梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#regression"><span class="toc-number">2.1.</span> <span class="toc-text">Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#closed-form-solution"><span class="toc-number">2.2.</span> <span class="toc-text">Closed-form Solution</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#regularized-least-squares-regression"><span class="toc-number">2.3.</span> <span class="toc-text">Regularized Least Squares Regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gradient-descent-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.4.</span> <span class="toc-text">Gradient Descent 随机梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="toc-number">2.4.1.</span> <span class="toc-text">随机梯度算法</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-linear-classifcation-and-stochastic-gradient-descent-%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BB-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95"><span class="toc-number">3.</span> <span class="toc-text">2 Linear Classifcation and Stochastic Gradient Descent 线性分类、支持向量机、随机梯度算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#binary-classification-%E4%BA%8C%E5%85%83%E5%88%86%E7%B1%BB"><span class="toc-number">3.1.</span> <span class="toc-text">Binary Classification 二元分类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#support-vector-machine-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA"><span class="toc-number">3.2.</span> <span class="toc-text">support vector machine 支持向量机</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%B6%85%E5%B9%B3%E9%9D%A2%E5%92%8C%E9%97%B4%E9%9A%94"><span class="toc-number">3.2.0.1.</span> <span class="toc-text">超平面和间隔</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#gradient-descent-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">3.3.</span> <span class="toc-text">Gradient Descent 梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#stochastic-gradient-descent-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">3.4.</span> <span class="toc-text">stochastic gradient descent 随机梯度下降</span></a></li></ol><li class="toc-item toc-level-1"><a class="toc-link" href="#3-logistic-regression-and-ensemble-methods-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="toc-number">4.</span> <span class="toc-text">3 Logistic Regression and Ensemble Methods 逻辑回归与集成学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#logistic-regression-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">4.1.</span> <span class="toc-text">Logistic Regression 逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%9F%BA%E6%9C%AC%E5%8E%9F%E7%90%86"><span class="toc-number">4.1.1.</span> <span class="toc-text">1. 逻辑回归的基本原理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%81%87%E8%AE%BE"><span class="toc-number">4.1.1.1.</span> <span class="toc-text">假设</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.1.1.2.</span> <span class="toc-text">逻辑回归模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87"><span class="toc-number">4.1.1.3.</span> <span class="toc-text">目标</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-number">4.1.2.</span> <span class="toc-text">2. 损失函数与优化</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="toc-number">4.1.2.1.</span> <span class="toc-text">优化方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E4%B8%8E%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C"><span class="toc-number">4.1.3.</span> <span class="toc-text">3. 模型评估与决策边界</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E8%BE%B9%E7%95%8C"><span class="toc-number">4.1.3.1.</span> <span class="toc-text">决策边界</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-%E6%AD%A3%E5%88%99%E5%8C%96"><span class="toc-number">4.1.4.</span> <span class="toc-text">4. 正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.1.5.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%9A%E7%B1%BB%E5%88%AB%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98%E7%9A%84%E5%AE%9A%E4%B9%89"><span class="toc-number">4.2.</span> <span class="toc-text">多类别分类问题的定义</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-overfitting-underfitting-regularization-and-cross-validation-%E8%BF%87%E6%8B%9F%E5%90%88-%E6%AC%A0%E6%8B%9F%E5%90%88-%E6%AD%A3%E5%88%99%E5%8C%96%E5%92%8C%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">5.</span> <span class="toc-text">4 Overfitting, Underfitting, Regularization and Cross-Validation 过拟合、欠拟合、正则化和交叉验证</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88%E4%B8%8E%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="toc-number">5.0.1.</span> <span class="toc-text">过拟合与欠拟合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%81%8F%E5%B7%AE-%E6%96%B9%E5%B7%AE%E6%9D%83%E8%A1%A1-bias-variance-tradeoff"><span class="toc-number">5.0.2.</span> <span class="toc-text">偏差 - 方差权衡 Bias-Variance Tradeoff</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="toc-number">5.0.3.</span> <span class="toc-text">交叉验证</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%8E%E9%9B%86%E6%88%90%E6%96%B9%E6%B3%95"><span class="toc-number">6.</span> <span class="toc-text">5 非线性机器学习与集成方法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">6.1.</span> <span class="toc-text">决策树</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A4%BA%E8%8C%83%E6%8A%8A%E7%B1%BB%E4%BC%BC%E6%89%93%E7%BD%91%E7%90%83%E7%9A%84%E8%A1%A8%E6%A0%BC%E8%BD%AC%E5%8C%96%E4%B8%BA%E5%86%B3%E7%AD%96%E6%A0%91"><span class="toc-number">6.1.1.</span> <span class="toc-text">示范：把类似打网球的表格转化为决策树</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-multiclass-classification-and-cross-entropy-loss-%E5%A4%9A%E7%B1%BB%E5%88%86%E7%B1%BB%E5%92%8C%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">7.</span> <span class="toc-text">5 Multiclass Classification and Cross-entropy Loss 多类分类和交叉熵损失函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-neural-networks-and-deep-learning-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">8.</span> <span class="toc-text">6 Neural Networks and Deep Learning 神经网络与深度学习</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-rnn-transformer-bert"><span class="toc-number">9.</span> <span class="toc-text">7 RNN、Transformer、BERT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0"><span class="toc-number">9.1.</span> <span class="toc-text">集成学习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#bagging"><span class="toc-number">9.1.1.</span> <span class="toc-text">Bagging</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#adaboostadaptive-boosting"><span class="toc-number">9.2.</span> <span class="toc-text">AdaBoost（Adaptive Boosting）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E8%81%9A%E7%B1%BB%E5%AD%A6%E4%B9%A0"><span class="toc-number">10.</span> <span class="toc-text">6 无监督学习：聚类学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8D%E7%82%B9k-means%E8%81%9A%E7%B1%BB"><span class="toc-number">10.1.</span> <span class="toc-text">重点：K-Means 聚类</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B1%82%E6%AC%A1%E8%81%9A%E5%90%88%E8%81%9A%E7%B1%BB-hierarchical-agglomerative-clustering"><span class="toc-number">10.2.</span> <span class="toc-text">层次聚合聚类 Hierarchical Agglomerative Clustering</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-neural-networks-and-deep-learning-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0"><span class="toc-number">11.</span> <span class="toc-text">7 Neural Networks and Deep Learning 神经网络与深度学习</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">11.1.</span> <span class="toc-text">前向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">11.2.</span> <span class="toc-text">反向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cnn-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">11.3.</span> <span class="toc-text">CNN 卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C"><span class="toc-number">11.3.1.</span> <span class="toc-text">图像卷积操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-rnn-transformer-bert"><span class="toc-number">12.</span> <span class="toc-text">8 RNN、Transformer、BERT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-number">12.1.</span> <span class="toc-text">强化学习</span></a></li></ol></li></div><div class="related panel pjax" data-title="Related"></div><div class="overview panel" data-title="Overview"><div class="author" itemprop="author" itemscope itemtype="http://schema.org/Person"><img class="image" itemprop="image" alt="Cai Junyuan" data-src="/images/avatar.jpg"><p class="name" itemprop="name">Cai Junyuan</p><div class="description" itemprop="description"></div></div><nav class="state"><div class="item posts"><a href="/archives/"><span class="count">62</span> <span class="name">posts</span></a></div><div class="item tags"><a href="/tags/"><span class="count">19</span> <span class="name">tags</span></a></div></nav><div class="social"><span class="exturl item github" data-url="aHR0cHM6Ly9naXRodWIuY29tL3NtYWxsY2p5" title="https:&#x2F;&#x2F;github.com&#x2F;smallcjy"><i class="ic i-github"></i></span> <a href="/2628035541@qq.com" title="2628035541@qq.com" class="item email"><i class="ic i-envelope"></i></a></div><ul class="menu"><li class="item"><a href="/" rel="section"><i class="ic i-home"></i>Home</a></li><li class="item"><a href="/about/" rel="section"><i class="ic i-user"></i>About</a></li><li class="item dropdown"><a href="javascript:void(0);"><i class="ic i-feather"></i>Posts</a><ul class="submenu"><li class="item"><a href="/archives/" rel="section"><i class="ic i-list-alt"></i>Archives</a></li><li class="item"><a href="/categories/" rel="section"><i class="ic i-th"></i>Categories</a></li><li class="item"><a href="/tags/" rel="section"><i class="ic i-tags"></i>Tags</a></li></ul></li></ul></div></div></div><ul id="quick"><li class="prev pjax"><a href="/2024/08/15/%E5%A4%A7%E4%BA%8C%E5%AD%A6%E5%B9%B4%E6%80%BB%E7%BB%93/" rel="prev" title="Previous Post"><i class="ic i-chevron-left"></i></a></li><li class="up"><i class="ic i-arrow-up"></i></li><li class="down"><i class="ic i-arrow-down"></i></li><li class="next pjax"><a href="/2024/09/04/%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F/" rel="next" title="Next Post"><i class="ic i-chevron-right"></i></a></li><li class="percent"></li></ul></div></div><div class="dimmer"></div></div></main><footer id="footer"><div class="inner"><div class="widgets"><div class="rpost pjax"><h2>Random Posts</h2><ul><li class="item"><div class="breadcrumb"></div><span><a href="/2024/07/27/DragonOS%E7%BD%91%E7%BB%9C%E5%AD%90%E7%B3%BB%E7%BB%9F%E5%AD%A6%E4%B9%A0/" title="DragonOS网络子系统学习">DragonOS网络子系统学习</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/05/27/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F--Deadlocks/" title="操作系统--Deadlocks">操作系统--Deadlocks</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/05/16/%E9%97%B4%E6%8E%A5%E6%80%A7%E5%8E%9F%E5%88%99/" title="间接性原则">间接性原则</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/07/18/%E5%87%BD%E6%95%B0%E4%B8%8A%E9%94%81/" title="函数上锁">函数上锁</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/11/26/%E8%AE%B0%E5%BD%95%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%BC%80%E5%8F%91%E4%B8%AD%E9%81%87%E5%88%B0%E7%9A%84bug/" title="Untitled">Untitled</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/07/18/%E7%8E%AF%E5%BD%A2%E5%88%97%E8%A1%A8%E7%9A%84%E6%8E%A2%E6%9F%A5/" title="环形列表的探查">环形列表的探查</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/07/27/IcmpSocket%E6%98%AF%E5%90%A6%E5%B1%9E%E4%BA%8E%E5%86%85%E6%A0%B8%E7%9A%84%E8%8C%83%E7%95%B4/" title="icmpSocket是否属于内核范畴">icmpSocket是否属于内核范畴</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/09/04/%20%E8%BD%AF%E4%BB%B6%E6%9E%B6%E6%9E%84%E5%AE%9E%E8%B7%B5/" title="软件架构实践">软件架构实践</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/07/27/%E5%86%85%E5%AD%98%E5%AE%89%E5%85%A8%E9%97%AE%E9%A2%98%E4%B9%8B%20use-after-free%20%E6%BC%8F%E6%B4%9E%E9%97%AE%E9%A2%98/" title="内存问题之 use-after-free 问题">内存问题之 use-after-free 问题</a></span></li><li class="item"><div class="breadcrumb"></div><span><a href="/2024/06/04/unixSock/" title="unix Stream Sock">unix Stream Sock</a></span></li></ul></div><div><h2>Recent Comments</h2><ul class="leancloud-recent-comment"></ul></div></div><div class="status"><div class="copyright">&copy; 2010 – <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="ic i-sakura rotate"></i> </span><span class="author" itemprop="copyrightHolder">Cai Junyuan @ SmallC</span></div><div class="powered-by">Powered by <span class="exturl" data-url="aHR0cHM6Ly9oZXhvLmlv">Hexo</span> & Theme.<span class="exturl" data-url="aHR0cHM6Ly9naXRodWIuY29tL2FtZWhpbWUvaGV4by10aGVtZS1zaG9rYQ==">Shoka</span></div></div></div></footer></div><script data-config type="text/javascript">var LOCAL={path:"2024/09/04/机器学习/",favicon:{show:"（●´3｀●）Goooood",hide:"(´Д｀)Booooom"},search:{placeholder:"Search for Posts",empty:"We didn't find any results for the search: ${query}",stats:"${hits} results found in ${time} ms"},valine:!0,fancybox:!0,copyright:'Copied to clipboard successfully! <br> All articles in this blog are licensed under <i class="ic i-creative-commons"></i>BY-NC-SA.',ignores:[function(e){return e.includes("#")},function(e){return new RegExp(LOCAL.path+"$").test(e)}]}</script><script src="https://cdn.polyfill.io/v2/polyfill.js"></script><script src="//cdn.jsdelivr.net/combine/npm/pace-js@1.0.2/pace.min.js,npm/pjax@0.2.8/pjax.min.js,npm/whatwg-fetch@3.4.0/dist/fetch.umd.min.js,npm/animejs@3.2.0/lib/anime.min.js,npm/algoliasearch@4/dist/algoliasearch-lite.umd.js,npm/instantsearch.js@4/dist/instantsearch.production.min.js,npm/lozad@1/dist/lozad.min.js,npm/quicklink@2/dist/quicklink.umd.js"></script><script src="/js/app.js?v=0.2.5"></script></body></html><!-- rebuild by hrmmi -->